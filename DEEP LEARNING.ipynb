{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bf569e",
   "metadata": {},
   "source": [
    "* Feed Forward propogation - humne input diya, it passes through a hidden layer and gives us the ouput\n",
    "* Back propogation - if desired output nahi aya toh model backwards run kare ga and will adjust weights to achieve the desired output. During this process model learns how to balance and set weights.This thing goes on (forward and backwards) until we achieve the desired result.\n",
    "* cost function/error  = actual value(jo desired thi) - predicted value(jo model ne predict ki)\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a4def",
   "metadata": {},
   "source": [
    "# Neural Network Architecture.\n",
    "\n",
    "### single layer perceptron model (SLP)\n",
    "* perceptron is fundamental unit\n",
    "* neural network has only input and output layer. no hidden layers.\n",
    "\n",
    "### Radial Basis Network (RBN)\n",
    "\n",
    "* composed of input, single hidden and then output layer\n",
    "* there are feed forward neural networks which only uses RBN as activation function\n",
    "* ACTIVATION FUNCTION - decides whether neuron should be activated or not.\n",
    "* in RBN , a RBF (Radial basis Function) is the activation function. It assigns a real value to each i/p and also the value produced by the RBF is always an absolute value.\n",
    "\n",
    "### Multi-layer Perceptron (MLP)\n",
    "* more than one hidden layer of neurons.\n",
    "* input , output and many hidden layers in between\n",
    "* also known as deep feed forward NN\n",
    "* hidden layers are the real computational engine of MLP NN \n",
    "\n",
    "### Recurrent Neural Network (RNN)\n",
    "* process memory \n",
    "* value of hidden layer neuron is a combination of data it recieves from lower layer and it's PREVIOUS VALUE.\n",
    "* previous value contributes to the current value and this feature is what enables the NN to process memory\n",
    "* RNN are neural networks that may be used to analyze time series data or data that involves sequences .\n",
    "* an ordinary feed forward NN you need to have a large number of independent data points, however if data is arranged in a way that each data point is dependent on the one before it, we will have to make changes to the NN to account for these interdependencies\n",
    "* the memory lets RNN's keep the track of prior input so they may create next new output in sequence\n",
    "\n",
    "#### Long Short-Term Memory (LSTM) RNN\n",
    "* is memory cell in incorporated in the hidden cell neurons, is called LSTM network. \n",
    "* LSTM is a type of RNN\n",
    "* LSTM is able to learn long term dependencies by the use of short term memory network.\n",
    "* by design, these do not suffer from the long term dependency issue. they don'r need to work hard on how to retain knowledge for lengthy periods of time.\n",
    "\n",
    "#### Hopfield Neural Network\n",
    "* special type of RNN.\n",
    "* different from other NN.\n",
    "* no. of neurons is the same as the number of inputs and outputs\n",
    "* we then have the same number of inputs and outputs. \n",
    "* Hopfield NN is a fully interconnected network of neurons in which each neuron is connected to every other neuron.\n",
    "* Network is trained with input patterns by setting a value of neurons equal to the value of the desired pattern.\n",
    "* then the weights are computed through deep learning algorithms.\n",
    "\n",
    "#### Boltzman Machine Neural Network\n",
    " * similar to hopfield except, some neurons are directly connectly to inputs while others are not associated to inputs directly.\n",
    " * weights are initialized randomly and learned through back propogation algorithm.\n",
    " * also a type of RNN\n",
    " * Composed of nodes that make binary judgments and are predisposed to particular biases\n",
    "* ** In this context, \"binary judgments\" means that each node can only choose between two options, often represented as \"yes\" or \"no,\" \"true\" or \"false,\" or \"1\" or \"0.\" The \"predisposed to particular biases\" aspect indicates that these nodes have inherent inclinations or preferences that influence their decision-making process, leading them to make certain choices more frequently or readily than others. The presence of biases can have significant implications on the outcomes and behavior of the overall system. Addressing biases is an essential concern in designing fair and ethical AI systems. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358935e",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "* helps neural network to learn complex patterns in the data.\n",
    "* activation function takes input from the output signal of previous neurons and them into some form that can be taken as input to the next neuron.\n",
    "* it takes some input and apply some function on that input to get a result and this result is transferred to next neurin.\n",
    "* activation function is also known as transfer function\n",
    "###### LINEAR ACTIVATION FUNCTION vs NON LINEAR ACTIVATION FUNCTIONS\n",
    "* NN with no activation function is linear in nature like polynomial of one degree or as a linear regression model.\n",
    "* linear functions have less power to learn complex functional mapping from data\n",
    "* one of the main purposes of activation function is to introduce non linearity to the output of a neuron bcz real world problems are non linear, high dimensional and complicated. \n",
    "* non linear activation function helps to mimic the real world accurately\n",
    "IMPORTANT TEMINNOLOGIES FOR NON LINEAR ACTIVATION FUNCTION\n",
    "* Differential Function - Change in y-axis w.r.t. change in X-axis(slope)\n",
    "* Monotonic Function - A function which is increasing on its entire domain or decreasing on its entire domain\n",
    "### Non Linear Activation Functions\n",
    "\n",
    "#### 1 -The Sigmoid Function\n",
    "*  range is limited to greater than 0 and less than 1. (0<range<1).\n",
    "* as range is zero to one, it is specially used for models where we have to predict probability as the output\n",
    "* its diffrentiable which means we can find slope between any two points/\n",
    "* it is monotonic function\n",
    "* it is not very useful in real world examples because of the following 3 reasons:\n",
    "* 1 - its computational inexpensive\n",
    "* 2- It is not zero centered which means the range only consists of either positive or negative values.\n",
    "* 3- It causes vanishing gradient problems: this problem is encountered when an Artificial Neural Network is trained with gradient based learning methods and back propogation. In such methods each of the NN which recieves an update proportional to its partial derivative of the error function with respect to the current weight in each iteration of training\n",
    "* ** in worst case the gradient value will be so small that it might not be able to change any weight.\n",
    "\n",
    "A sigmoid function will always output a value between 0 and 1 regardless of any real number input. Basically, Any real number input will be converted to a number between 0 and 1. hence, sigmoid function is said to squeeze values.\n",
    "\n",
    "#### 2- Hyperbolic Tangent Function\n",
    "* better version of sigmoid function bcz it ranges from -1 to 1.\n",
    "* it is also a differentiable function and a monotonic function.\n",
    "* There are applications where the hyperbolic tangent function is advantageous since it falls below zero\n",
    "* e.g. negative inputs are much strongly negative and zero inputs are mopped near zero in the hyperbolic tangent graph\n",
    "![](HyperbolicTangentGraph.png)\n",
    "* this is an advantage because it maps the -ve inputs extremely negative. \n",
    "* Hyperbolic Tangent Funtion is mainly used for classification between 2 classes\n",
    "* Hyperbolic Tangent Funtion is used in feed forward propogation in order to increase difference between outputs.n \n",
    "\n",
    "#### 3- Softmax Function\n",
    "* this particular activation function works in the output layer\n",
    "* it applies a standard exponential function to all the values in the output layer. then it does the normalization by dividing each value by the sum of all exponentials. so this way all values sum up to 1\n",
    "* computes the probability or confidence score for each attribute,so it is used in multiclass classification problems\n",
    "* it is also a differentiable function\n",
    "\n",
    "#### 4- Recified Linear Unit (ReLU) function \n",
    "* if the value is less than 0, the function becomes zero\n",
    "* if value is equal to zero or greater than zero, the function becomes the number.\n",
    "* commonly used in Convolutional Neural Networks, as well as in deep learning approached bcz it doesn't cause vanishing gradient problems\n",
    "* it is also differentiable function\n",
    "* not a monotonic function\n",
    "* range of this activation function is zero to infinity.\n",
    "* One problem with this function is that we lose data that is less than zero and our NN learns nothing from those neurons.\n",
    "* moreover, it maps all the values lesser than zero to zero\n",
    "* not proper appraoch bcz it makes our Neural Network unstable\n",
    "\n",
    "#### 5- Leaky Recified Linear Unit (ReLU) function \n",
    "* modification of the ReLU function, it solves the negative impact of of the ReLU function.\n",
    "* Usually the value for 'a' in diagram is around 0.01.\n",
    "![](LeakyReLU.png)\n",
    "* range of Leaky ReLU is from negative infinity to positive infintiy\n",
    "* differentiable and a monotonic function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcfd84",
   "metadata": {},
   "source": [
    "## Gradient Decent\n",
    "How network adjust its weights and learn by Itself?\n",
    "\n",
    "* most popular optimization algorithm in ML and DL\n",
    "* it finds the values of a function's parameters, coefficients that minimize the cost function as far as possible.\n",
    "* You start by defining the initial parameter values and from there gradient decent uses calculus to iteratevly adjust he values so that they minimize the given cost function\n",
    "![](GradientDecent.png)\n",
    "* assume this graph is cost function of our model. Blue dots are different iterations where different weight adjusted on each iteration.\n",
    "* we will get a different cost function on each point because it is changing weights after each iteration. \n",
    "* It takes initial value of the function and applies the formula shown:\n",
    "![](GDFormula.png)\n",
    "* here f(a)= laplace operator function of 'a'.\n",
    "* from current position we either move forward or backward, and the size of our step is instructed by the gamma laplace of the function of a.\n",
    "* we can also assume that the size of the step is defined by the value of the gradient or slope.\n",
    "* high gradient = larger step size\n",
    "*after some steps we will be able to reach the minimum value of  cost function.\n",
    "\n",
    "## Stochastic Gradient Decent (SGD)\n",
    "* main drawback of gradient decent is that it is limited to a convex function in terms of shape of teh function.\n",
    "* for a non convex shape as shown, gradient decent wont produce the desired results bcz we have multiple minimal values, lcal minimal values and in this case, gradient decent might not find the optimal minima (Global Minimum) from the shape.\n",
    "* for this kind of situation we opt for the STOCHASTIC GRADIENT DECENT OPTIMIZATION TECHNQUE\n",
    "![](StochasticDecent.png)\n",
    "\n",
    "* as name stichastic represents, this technique has some randomness\n",
    "* in SGD technique. we find out the gradient of the cost function of a single input at each iteration instead of the sum of the gradient of the cost function of all inputs\n",
    "* with SGD technique, since only one sample from the dataset is chosen at random for each iteration, the path taken by the algorithm to reach the minima is usually noisier than typical gradient decent algorithm\n",
    "* path taken by the algorithm does not matter as long as we reach the minimaand with a significantly shorter training time.\n",
    "* typically SGD takes a random single value at a time\n",
    "\n",
    "if algorithm takes a subset of values rather than a single value, in this case, it is known as mini batch gradient decent\n",
    "\n",
    "* One imporant difference between GD and SGD is that in gradient decent for every iteration, we take the whole dataset to measure the gradient and obtain parameters of the cost function. While in SGD we take a single value or a subset of values in iteration to measure gradient and then to update the parameters of cost function.\n",
    "* so if the data is very large, gradient decent may take too long. In this case SGD would be the faster\n",
    "![](SGD2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5c881",
   "metadata": {},
   "source": [
    " # How ANN works?\n",
    "    \n",
    "    * diference un ML and DL is the presence of hidden layer in DL\n",
    "   \n",
    "   * ![](ANN.png)\n",
    "   * in this example the activation function is sigmoid\n",
    "   \n",
    " ### advantages of ANN \n",
    " * have the ability to worl with incomplete knowledge. After the ANN is trained, the data may produce output even with incomplete information. Loss of performance here depends on the importance of missing information.\n",
    " * Fault Tolerance - corruption of one or more cells of the ANN will not stop the generation of output\n",
    " * parallel performance capability - the ANN can perform more than one job at a time\n",
    " \n",
    " ### disadvantges of the ANN\n",
    " * hardware dependency - ANN require processors with parallel processing power in accordance with their structure. for this reason the realization of equipment is dependent.\n",
    " * Determination of proper network structure -  There is no specific rule for determining the structure of ANN. Appropriate network structure is achieved through experience as well as through trial and error.\n",
    " \n",
    " # applications of ANN\n",
    " * handwritten recognition\n",
    " * image compression\n",
    " * stock exchange prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e203334",
   "metadata": {},
   "source": [
    "# Using ANN for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c6bb4",
   "metadata": {},
   "source": [
    "# problem statement - To see if customers leave the bank or stay.\n",
    "* creating a predictive model to predict behaviour of new customers, whether he or she will stay in the bank or leave the bank so that the bank can offer something special for the customer, about whom our model predicted, will leave the bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a55e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67bfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    " data = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce8e8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0             1    15634602   Hargrave          619    France  Female   42   \n",
       "1             2    15647311       Hill          608     Spain  Female   41   \n",
       "2             3    15619304       Onio          502    France  Female   42   \n",
       "3             4    15701354       Boni          699    France  Female   39   \n",
       "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
       "...         ...         ...        ...          ...       ...     ...  ...   \n",
       "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
       "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
       "9997       9998    15584532        Liu          709    France  Female   36   \n",
       "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
       "9999      10000    15628319     Walker          792    France  Female   28   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          2       0.00              1          1               1   \n",
       "1          1   83807.86              1          0               1   \n",
       "2          8  159660.80              3          1               0   \n",
       "3          1       0.00              2          0               0   \n",
       "4          2  125510.82              1          1               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "9995       5       0.00              2          1               0   \n",
       "9996      10   57369.61              1          1               1   \n",
       "9997       7       0.00              1          0               1   \n",
       "9998       3   75075.31              2          1               0   \n",
       "9999       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "...               ...     ...  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbe0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x - independent variables\n",
    "# y- dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4aa74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,3:13].values\n",
    "y = data.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2caf696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e861dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b49e2476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "# transforming object values to numeric values\n",
    "# geography - hot encoding using sklearn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_x_2  = LabelEncoder()\n",
    "x[:,2] = labelencoder_x_2.fit_transform(x[:,2])\n",
    "#fit_transform - maps string values to numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29c6d29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1  2   3   4          5  6  7  8          9\n",
       "0     619   France  0  42   2        0.0  1  1  1  101348.88\n",
       "1     608    Spain  0  41   1   83807.86  1  0  1  112542.58\n",
       "2     502   France  0  42   8   159660.8  3  1  0  113931.57\n",
       "3     699   France  0  39   1        0.0  2  0  0   93826.63\n",
       "4     850    Spain  0  43   2  125510.82  1  1  1    79084.1\n",
       "...   ...      ... ..  ..  ..        ... .. .. ..        ...\n",
       "9995  771   France  1  39   5        0.0  2  1  0   96270.64\n",
       "9996  516   France  1  35  10   57369.61  1  1  1  101699.77\n",
       "9997  709   France  0  36   7        0.0  1  0  1   42085.58\n",
       "9998  772  Germany  1  42   3   75075.31  2  1  0   92888.52\n",
       "9999  792   France  0  28   4  130142.79  1  1  0   38190.78\n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f2b0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c70fae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([('ohe',OneHotEncoder(),[1])], remainder = 'passthrough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da597eb",
   "metadata": {},
   "source": [
    "*  first augment in square is an array called transformers, which is a list of tuples. the array has the collowing element in the same order: \n",
    "1) A name for the column transformer i.e. 'ohe' in this case which will make setting of parameters and searching of trandformers easy\n",
    "2) Transformer : Here we are supposed to provide an estimator. we can also just passthrogh or drop if we want, but since we are encoding the data in this example we will use OneHotEncoder() here. Remember the estimator you use here needs to support fit and transform\n",
    "3) list of columns that you want to be transformed. in this case the 2nd column i.e. [1].\n",
    "\n",
    "* the second parameter we are interested in is the remainder this will tell the trandformer what to do with the other columns in tha dataset. By default, only the columns that are transformed will be returned by the transformer. all other columns will be dropped. But we have the option to tell the transformer what to do with the other columns: we can either drop them, pass them through unchanged or specify another estimator if you want to do some more processing, in this example we made sure all the remainder columns are allowed to pass through without any changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aedb3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danish\\AppData\\Local\\Temp\\ipykernel_26936\\3306601298.py:1: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.array(ct.fit_transform(x), dtype = np.str)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(ct.fit_transform(x), dtype = np.str)\n",
    "\n",
    "# takes all values from transformed dataset and convert it to an array. this array consit of the list of each row and and all of the values are converted to string using dtype = np.str augment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d820a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x[:,1:]\n",
    "\n",
    "# we are ignoring the first index bcz the same information can be achieved from index 1 and 2 that is colum 0 and 1 in the x printed below.\n",
    "# column number 0 and 1 ka combination apko bata raha hai k konsi country hai or dataframe x jo phele print kia tha uska column 1 bh same information de rha tha. isiliye humne usko hata k encoded wale columns rehne diye\n",
    "#we managed to reduce one input neuron without the loss of any information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48abf50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>771</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>516</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>709</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>772</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>792</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2  3   4   5          6  7  8  9          10\n",
       "0     0.0  0.0  619  0  42   2        0.0  1  1  1  101348.88\n",
       "1     0.0  1.0  608  0  41   1   83807.86  1  0  1  112542.58\n",
       "2     0.0  0.0  502  0  42   8   159660.8  3  1  0  113931.57\n",
       "3     0.0  0.0  699  0  39   1        0.0  2  0  0   93826.63\n",
       "4     0.0  1.0  850  0  43   2  125510.82  1  1  1    79084.1\n",
       "...   ...  ...  ... ..  ..  ..        ... .. .. ..        ...\n",
       "9995  0.0  0.0  771  1  39   5        0.0  2  1  0   96270.64\n",
       "9996  0.0  0.0  516  1  35  10   57369.61  1  1  1  101699.77\n",
       "9997  0.0  0.0  709  0  36   7        0.0  1  0  1   42085.58\n",
       "9998  1.0  0.0  772  1  42   3   75075.31  2  1  0   92888.52\n",
       "9999  0.0  0.0  792  0  28   4  130142.79  1  1  0   38190.78\n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x)\n",
    "\n",
    "# first 2 columns together are hot encoded for geography. e.g 0 0 means france, 0 1 means spain and 1 0 means germany "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79c17514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2, random_state = 0)\n",
    "\n",
    "# random state = 0 tells pur method not to randomly chose rows so that the corresponsing values in all the variables match the same row and if this code is run again, it should choose the same rows as the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c1db9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SCALING\n",
    "# technique used to standarize the independent feature present in the data in a fixed range\n",
    "# e.g. in out data some ranges are 0 to 1, some are 0 to 100, some are 0 to 100000. These huge ranges reqire a lot of time for calculation\n",
    "#to vercome this problem we perform feature scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f28fa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0709e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "# here we used fit_transform on training set bcz we learn the parameters of scaling on the training data and at the same time we scale the training data\n",
    "# now for the test set we only use transform bcz we want to use the scaling parameters learnt on the training set. there is no need for fir_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1810034f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>0.169582</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.464608</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>-1.215717</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.106432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.754865</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-2.304559</td>\n",
       "      <td>0.916013</td>\n",
       "      <td>0.301026</td>\n",
       "      <td>-1.377440</td>\n",
       "      <td>-0.006312</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>-0.748664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-1.191196</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.943129</td>\n",
       "      <td>-1.031415</td>\n",
       "      <td>0.579935</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.485335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>0.035566</td>\n",
       "      <td>0.916013</td>\n",
       "      <td>0.109617</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.473128</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.276528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>2.056114</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>1.736588</td>\n",
       "      <td>1.044737</td>\n",
       "      <td>0.810193</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>0.558378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>1.754865</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-0.582970</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.656016</td>\n",
       "      <td>-0.339364</td>\n",
       "      <td>0.703104</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>1.091330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>1.478815</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-1.613058</td>\n",
       "      <td>-0.339364</td>\n",
       "      <td>0.613060</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>0.131760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>0.916013</td>\n",
       "      <td>-0.368904</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>1.361474</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.412320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>-0.624205</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.081791</td>\n",
       "      <td>1.390762</td>\n",
       "      <td>-1.215717</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>0.844321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>1.754865</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-0.284011</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>0.875251</td>\n",
       "      <td>-1.377440</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>0.324725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.569844  1.743090  0.169582 -1.091687 -0.464608  0.006661 -1.215717   \n",
       "1     1.754865 -0.573694 -2.304559  0.916013  0.301026 -1.377440 -0.006312   \n",
       "2    -0.569844 -0.573694 -1.191196 -1.091687 -0.943129 -1.031415  0.579935   \n",
       "3    -0.569844  1.743090  0.035566  0.916013  0.109617  0.006661  0.473128   \n",
       "4    -0.569844  1.743090  2.056114 -1.091687  1.736588  1.044737  0.810193   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7995  1.754865 -0.573694 -0.582970 -1.091687 -0.656016 -0.339364  0.703104   \n",
       "7996 -0.569844  1.743090  1.478815 -1.091687 -1.613058 -0.339364  0.613060   \n",
       "7997 -0.569844 -0.573694  0.901515  0.916013 -0.368904  0.006661  1.361474   \n",
       "7998 -0.569844  1.743090 -0.624205 -1.091687 -0.081791  1.390762 -1.215717   \n",
       "7999  1.754865 -0.573694 -0.284011 -1.091687  0.875251 -1.377440  0.511364   \n",
       "\n",
       "            7         8         9         10  \n",
       "0     0.809503  0.642595 -1.032270  1.106432  \n",
       "1    -0.921591  0.642595  0.968738 -0.748664  \n",
       "2    -0.921591  0.642595 -1.032270  1.485335  \n",
       "3    -0.921591  0.642595 -1.032270  1.276528  \n",
       "4     0.809503  0.642595  0.968738  0.558378  \n",
       "...        ...       ...       ...       ...  \n",
       "7995  0.809503  0.642595  0.968738  1.091330  \n",
       "7996 -0.921591  0.642595  0.968738  0.131760  \n",
       "7997  0.809503  0.642595 -1.032270  1.412320  \n",
       "7998  0.809503  0.642595  0.968738  0.844321  \n",
       "7999 -0.921591  0.642595 -1.032270  0.324725  \n",
       "\n",
       "[8000 rows x 11 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fa1f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building ANN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# importing sequential class bcz it allows us to build an ANN but as a sequence of layers\n",
    "# ANN are built with fully connected layers. in order to implement the ANN,we need to initizalize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "faa31736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we start by creating object of our sequential class\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6aa7f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db1f2ed",
   "metadata": {},
   "source": [
    "* This is going to add a further component of networks to our model\n",
    "* in the above code, add method takes input from the dense class. Here dense creates a fully connected 2 layer network. This is composed of 11 input neurons and 6 output neurons.\n",
    "* we will add more layers like this so that the six output neurons will be the hidden layer of out complete network.\n",
    "* the parameters for dense include: \n",
    "* 1) units (represents the number of last layer neurons for now), \n",
    "* 2) kernel_intializer (the neural network needs to start with some weights and then iteratively update them to better values). The term kernel_intializeris fancy term for which a statistial districution or a function is used to initialize the weights. In case of a statistical distribution, the library will generate numbers from the statistical distribution and use them as starting weights. In our code the value for this parameter is uniform. this means that all weights will be uniformly distributed in our network at first\n",
    "* 3) activation - this refers to the activation function, we gave used the rectifier function RELU.\n",
    "* 4) input_dim - dimensions of input neurons, in out case it is 11 bcz we have 11 independent variables (inclusing 2 columns of geography)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4fa0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding next hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f61103",
   "metadata": {},
   "source": [
    "*  same code as previous one, we just dont need the input_dim parameter because it automatically connects to the previous layer of our network. \n",
    "* so 6 neurons from the previous layer will be input to this layer.\n",
    "* until now we have one input layer and 2 hidden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0307436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaba9ee",
   "metadata": {},
   "source": [
    "* in output layer we need just one neuron bcz as you can see in the dataset we have dependent variable in binary form (0 -customer stay or 1- customer leave)\n",
    "* unit parameter is 1 and actication function is sigmoid (bcz sigmoid activation function provides the probability of a customer staying in the bank or leaving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1ea27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the ANN\n",
    "# we need to train and tune the ANN according to our data\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1233b7a",
   "metadata": {},
   "source": [
    "*  compile is a method of the Tensor Flow library while compiles the ANN together.\n",
    "* first parameter is the optimizer. its value in our code is adam. this perform the stochastic gradient decent optimizer technique .\n",
    "* next is the loss parameter, which represents the loss function. It is an optimization function which is used in case of training a classification model which classifies the data by predicting the probability of whether the data belongs to one class or the other.\n",
    "* one thing we need to be aware of while doing a binary prediction similar to this one, always use the loss function as 'binary_crossentropy' . the binary_crossentropy function computes the cross entropy loss between the true and predicted levels.\n",
    "* third parameter is about evaluation of our network. To evualte our ANN we have used the accuracy metrics. Accuracy metrics calculates how often prediction equal labels. This matrix create 2 local variables: total and count that are used to compute the frequency with which y_pred matches y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ee86258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4854 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4277 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4212 - accuracy: 0.8008\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4178 - accuracy: 0.8231\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4155 - accuracy: 0.8270\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4138 - accuracy: 0.8291\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4128 - accuracy: 0.8319\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4118 - accuracy: 0.8328\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4102 - accuracy: 0.8326\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4097 - accuracy: 0.8341\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4088 - accuracy: 0.8338\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4078 - accuracy: 0.8338\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4074 - accuracy: 0.8330\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4067 - accuracy: 0.8338\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4065 - accuracy: 0.8351\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8345\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4056 - accuracy: 0.8330\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4050 - accuracy: 0.8354\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4049 - accuracy: 0.8345\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4045 - accuracy: 0.8338\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4042 - accuracy: 0.8351\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4040 - accuracy: 0.8338\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4036 - accuracy: 0.8355\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4038 - accuracy: 0.8329\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4036 - accuracy: 0.8354\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8339\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4029 - accuracy: 0.8353\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4035 - accuracy: 0.8357\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4031 - accuracy: 0.8342\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4027 - accuracy: 0.8344\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4027 - accuracy: 0.8354\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4032 - accuracy: 0.8335\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4026 - accuracy: 0.8366\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8341\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4022 - accuracy: 0.8342\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4021 - accuracy: 0.8339\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8353\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8342\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4014 - accuracy: 0.8351\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4018 - accuracy: 0.8346\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4018 - accuracy: 0.8344\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4017 - accuracy: 0.8364\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4017 - accuracy: 0.8339\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4007 - accuracy: 0.8341\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4014 - accuracy: 0.8336\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4014 - accuracy: 0.8361\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4015 - accuracy: 0.8336\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4012 - accuracy: 0.8351\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4012 - accuracy: 0.8335\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4014 - accuracy: 0.8344\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4014 - accuracy: 0.8341\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4015 - accuracy: 0.8339\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4016 - accuracy: 0.8351\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4013 - accuracy: 0.8335\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4013 - accuracy: 0.8347\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4011 - accuracy: 0.8339\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4012 - accuracy: 0.8340\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4011 - accuracy: 0.8342\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4010 - accuracy: 0.8344\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8340\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4012 - accuracy: 0.8339\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4006 - accuracy: 0.8342\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4007 - accuracy: 0.8346\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8330\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8346\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4012 - accuracy: 0.8334\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4005 - accuracy: 0.8335\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4005 - accuracy: 0.8342\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8344\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4011 - accuracy: 0.8338\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4010 - accuracy: 0.8326\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4006 - accuracy: 0.8344\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8346\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4007 - accuracy: 0.8353\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4002 - accuracy: 0.8356\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4004 - accuracy: 0.8354\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4006 - accuracy: 0.8354\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4007 - accuracy: 0.8345\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4003 - accuracy: 0.8363\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4006 - accuracy: 0.8341\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4010 - accuracy: 0.8353\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3998 - accuracy: 0.8353\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4006 - accuracy: 0.8356\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4004 - accuracy: 0.8351\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8342\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4003 - accuracy: 0.8354\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4002 - accuracy: 0.8351\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4008 - accuracy: 0.8355\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4006 - accuracy: 0.8355\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4002 - accuracy: 0.8345\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4002 - accuracy: 0.8350\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4002 - accuracy: 0.8336\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4003 - accuracy: 0.8354\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4002 - accuracy: 0.8355\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4003 - accuracy: 0.8361\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4004 - accuracy: 0.8361\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4002 - accuracy: 0.8353\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8355\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4003 - accuracy: 0.8349\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dae280d790>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the ANN model to the training set\n",
    "classifier.fit(x_train,y_train,batch_size = 10, epochs  = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ad52c",
   "metadata": {},
   "source": [
    "* 1st we have values for input neurons, then we have values for output layer neurons\n",
    "* 3rd augment is the batch_size- instead of comparing out results with real results one by one, its better to perform this task in a batch, so out code takes 10 rows at a time\n",
    "* last augment is epochs, epochs is the number of times an algorithm visits the dataset. In other words, epoch is one backward and one forward pass of all the training examples.\n",
    "* the neural network has to train on a certain number of epochs to improve the accuracy over time\n",
    "* we can see accuracy of each epochs improving over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00660a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# predicting the test set results\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947a418",
   "metadata": {},
   "source": [
    "* we are defining that if y_pred is in between 0 and 0.5 then this new y_pred will become 0 i.e. false.\n",
    "* is y_pred is larger than 0.5 then the new y_pred will become 1 i.e. True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "911799b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfa23d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07c940f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1595    0]\n",
      " [ 405    0]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8a7e8",
   "metadata": {},
   "source": [
    "* a confusion matrix is also called as an error matrix. it is summary of prediction results on a classification problem.\n",
    "* e.g.\n",
    "* [[1499 96]\n",
    "* [189 216]]\n",
    "\n",
    "* 1499 true values are correctly predicted true\n",
    "* 96 false values are incorrectly predicted true.\n",
    "* 189 true values are incorrectly predicted False.\n",
    "* 216 false values are correctly predicted false.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eefac544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7975"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f9bac",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (ConvNet)\n",
    "* very important part of many computer vision applications\n",
    "* It is a class of neural networks most commonly applied to analyze images.\n",
    "* nomrally when we think of nn we think about matrix multiplication but that is not the case with CNN.\n",
    "* it uses a special technique called convolution. it is a mathematical operation on two functions. when two function e.g. F and G are combined mathematically, the result is a third function that expresses that how the shape of one is modified by the shape of other.\n",
    "* in mathematics and especially functional , convolution is a matematical function on 2 functions F and G that results in a third function that expresses how the shape of one is modified by the shape of the other.\n",
    "* the word convolution refers to both of the outcome function and the technique of calculating it and it is used inter changeably.\n",
    "*  after one function is reversed and shifted, the convolution is defined as the integral of the product of the two functions after they've been combined together . The integral is calculated for all possible values of shift resulting in the convolution function being produced\n",
    "* In a CNN, our input image can be either grid scale or colorful\n",
    "* image is made of pixels range from 0 to 255. we need to normalize them i.e. convert the range between 0 and 1 before passing it to the model.\n",
    "### Components of a CNN in detail\n",
    "* works in 2 steps i.e. Feature Extraction and Classification\n",
    "![](CNN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f3e0d",
   "metadata": {},
   "source": [
    "#### Convolution layer\n",
    "\n",
    "* convolution layer is the layer where the filter is applied to our input image to extract or detect its features. \n",
    "* A filter is applied to image multiple times and creates a feature map which helps in classifying the input image\n",
    "* 6 by 6 ki image pey 3 by 3 ka filter lagaya. result is a feature map of 4 by 4 that is  which has some ingormation about the input image\n",
    "![](ConvLayer.png)\n",
    "* in real life many filters are applied to image to extract information\n",
    "* as in the figure, filter is applied to the green highlighted part of the image.\n",
    "* pixel values of the image are multiplied by the values of filter and then summed up to get the final value. \n",
    "* in nest step the filter is shifted by one column. This jump to the next column or row is called as Stride.\n",
    "* stride of 1 means we are shifting by one column\n",
    "* once we get the feature map, an activation function is applied to it to introduce non linearity\n",
    "* activation funtion ina neural network defines how the weighted sum of the inputs is transformed into a output from a node or nodes in layer of the network.\n",
    "* Feature map that we get here is smaller than the size of our image when the center of our filter sits on a value to change other the values of the filter must also sit on some values. in our case the center of the filter cannot sit on the outermost values of our image. So, we have lost a layer from each side.\n",
    "* if our filter is  5 by 5 then we lose two outer most layers from all sides.\n",
    "* as we increase the value of stride, the size of the features map decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbe830",
   "metadata": {},
   "source": [
    "### Pooling layer\n",
    "![](PoolingLayer.png)\n",
    "\n",
    "\n",
    "* here in this image we are doing max pooling of 2 by 2 with a stride of 2. \n",
    "* pooling is a method to convert an input image to a lower resolution version, which still contains the large or important elements of the input image\n",
    "* the most common pooling types are max pooling and average pooling\n",
    "* maxpooling extracts the maximum value of the area it convolves.\n",
    "* average pooling computes thea average of the value of the area it convolves. \n",
    "* Average pooling method smooths out the image and hence the sharp features may not be identified when this poooling method is used\n",
    "* Max pooling selects the brighter pixels from the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278079ab",
   "metadata": {},
   "source": [
    "\n",
    "### Fully Connected layer\n",
    "* until now we have performed all the feature extraction steps\n",
    "* convolution + pooling combinely extract different features from an image \n",
    "* for example, the edges of a shape in an image can be detected. If image is of a face, then eyes, nose and mouth can be detected.\n",
    "* After convolution + pooling layers we add a couple of fully connected layers to wrap up the CNN architecture\n",
    "* this is the same fully connected ANN architecture that we had previously in ANN\n",
    "![](CNNarch.png)\n",
    "* the output of both Convolution and the pooling layers are 2 dimensional, but a fully connected layer expects a 1 D vectors of numbers. So we flatten the output of final pooling layer to a vector, and that becomes the input to the fully connected layer.\n",
    "* Flattening - arranging 2D numbers into a 1D vector.\n",
    "* These Fully connected layers connect the information extracted from the previous steps i.e. convolution layers and the pooling layers to the output layer and eventually classifies the input image into the desired label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c1457",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION\n",
    "\n",
    "### MNIST Handwritten Digit Data\n",
    "* all images have the same square size i.e. 28 x 28 pixels\n",
    "* image are gray scale\n",
    "* all are handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b9cfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporing libraries\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential # model type (Sequential is the easiest way to build a model in keras. Allows u to build model layer by layer)\n",
    "from tensorflow.keras.layers import Conv2D # becz we are dealing with 2D images.2 dimensional convolution to help us in Convolution layers\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deafd0d",
   "metadata": {},
   "source": [
    "#### building the CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12b16624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 6s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# here we are loading the data\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a9498",
   "metadata": {},
   "source": [
    "* resize all images to specific size and color.\n",
    "* although in this example all images are grayscale and of same size i.e. 28 by 28 pixels\n",
    "* in real life it is not the case, color and size of images are different\n",
    "* we do so by the reshape method which take 4 augments\n",
    "* 1st augment is for the number of images\n",
    "* 2nd is the number of pixels in column of image\n",
    "* 2rd is the number of pixels in the row.\n",
    "* 4th is the augment that represents the color \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87b30d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the data\n",
    "x_train = x_train.reshape((x_train.shape[0]),x_train.shape[1],x_train.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2da0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((x_test.shape[0]),x_test.shape[1],x_test.shape[2],1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db21b4b",
   "metadata": {},
   "source": [
    "The line of code you provided is reshaping a 4-dimensional array `x_test` to add an additional dimension of size 1. This operation is typically done when working with Convolutional Neural Networks (CNNs) for image processing tasks.\n",
    "\n",
    "Let's break down the code step by step:\n",
    "\n",
    "1. `x_test` is the input data for testing. It is assumed to be a 4-dimensional array, where the dimensions represent the following:\n",
    "   - Dimension 1: Number of samples in the test data (the batch size).\n",
    "   - Dimension 2: Height of each sample image.\n",
    "   - Dimension 3: Width of each sample image.\n",
    "   - Dimension 4: Number of channels in each sample image. For grayscale images, this is 1; for RGB images, this is 3 (R, G, and B channels).\n",
    "\n",
    "2. `x_test.shape` returns a tuple representing the shape of the `x_test` array. For example, if `x_test` has a shape of (1000, 28, 28, 3), it means there are 1000 images in the test set, each having a size of 28x28 pixels, and each image has 3 channels (RGB).\n",
    "\n",
    "3. `x_test.shape[0]` refers to the first dimension of the `x_test` array, which is the number of samples in the test data (batch size). Similarly, `x_test.shape[1]` and `x_test.shape[2]` refer to the height and width of each sample image, respectively.\n",
    "\n",
    "4. `x_test.reshape((x_test.shape[0]), x_test.shape[1], x_test.shape[2], 1)` reshapes the `x_test` array by adding an extra dimension at the end with size 1. This effectively converts the array into a 4-dimensional array with the same number of samples, height, and width, but now each sample image has only one channel.\n",
    "\n",
    "The reason for adding this extra dimension is that many CNN architectures expect input data with four dimensions, including a single channel for grayscale images or three channels for RGB images. The additional dimension allows the CNN to process the data correctly and apply convolution operations across the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b00b71f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# here we are checking the shape after reshaping\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# out of 70000 images, 60000 are given for training and 10000 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60e26ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are normalizing the pixel values\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57298971",
   "metadata": {},
   "source": [
    "* we know that the pixel values in for each image in the dataset are unsigned integers in the  range of black and white (0 -255).\n",
    "* some scaling is required so a good start is to normalize the pixel values of grayscale images .e.g rescaling them to the range 0 to 1.\n",
    "* this involves first converting the data type from unsigned integers to floats then dividing the pixel values by the max value 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e99b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n",
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3eaafb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here when we load x_train and x_test, will contain the images\n",
    "#and y_train and y_test will contain the digits that those images represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69b3f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "298b5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# the model has two main aspects, the feature extraction (convolution + pooling layer)\n",
    "# and they classify a fully connected layers that will make a prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8cdd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a convolutional layer\n",
    "# here we are specifying 32 fill this size of 3 by 3\n",
    "# we are using rectified linear unit activation function in our layer\n",
    "# moreover input image is the size of 28 by 28 and they are grayscale\n",
    "\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu', input_shape = (28,28,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "959780d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now appling the max pooling layer\n",
    "# size of maxpooling is 2 by 2 which means it will take the maximum value from the 2 by 2 matix of pixels\n",
    "\n",
    "model.add(MaxPool2D(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6052c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fully connected layer\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100,activation = 'relu'))\n",
    "# we are adding dense layer of 100 nuerons in our output layer using the relu activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c1de4",
   "metadata": {},
   "source": [
    "* flatten serves as connection between the convolution and dense layers\n",
    "* arranges the pixel values from 2D images to 1D array\n",
    "* dense is the layer type we will use for out output layer\n",
    "* dense is a standard layer type that is used in many cases for neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "152837d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding output layer\n",
    "\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdbcaa",
   "metadata": {},
   "source": [
    "* here we are adding another dense layer of 10 neurons in our output layer bcz we know that there are only 10 digits (0-9)\n",
    "* activation function is softmax.\n",
    "* softmax makes the output sum upto 1 so the output may be interpreted as probabilities\n",
    "* the model will then make its prediction based on the option that has highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "789430d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss= 'sparse_categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b13c1",
   "metadata": {},
   "source": [
    "* So once our network is ready, we need to compile it in the same way as we did in our previous activity.\n",
    "\n",
    "* The only difference we have here is that we are using sparse, categorical cross entropy loss function instead of binary cross entropy loss function because this is a categorical problem, while the previous coding exercise was a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "600735eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 96s 49ms/step - loss: 4.6208 - accuracy: 0.1043\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 89s 48ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 89s 48ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 89s 47ms/step - loss: 4.6051 - accuracy: 0.0987\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 90s 48ms/step - loss: 4.6051 - accuracy: 0.0987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dae6339940>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(x_train,y_train,epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4288b29",
   "metadata": {},
   "source": [
    "* So to fit our dataset in our model, we are using the fit method. Moreover, we are specifying epochs as a 10, which means that our model will go through our dataset ten times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e42a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 15ms/step - loss: 4.6052 - accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.60518217086792, 0.09799999743700027]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47304e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
